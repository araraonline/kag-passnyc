{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring the applicants gaps in 2017\n",
    "\n",
    "Let's do something really simple, but in a really effective manner. That thing is: measuring the applicants gaps in 2017.\n",
    "\n",
    "**image of actual amount of applicants vs. expected amont of applicants**\n",
    "\n",
    "Each year, 8th graders from all over New York City take a test called SHSAT to see who will get access to the famous Specialized High Schools (SPHS). These Specialized High Schools are schools that receive great attention and provide a estimulating environment for students who have the desire to learn.\n",
    "\n",
    "Problem is, students that receive offers from SPHS are usually from a very specific demographic. Usually white or asian guys, mostly from the upper class, and from very small number of schools. This demographics characteristics, however, are tightly linked to the academic performance [1], showing deeply ingrained questions that cannot be solved by simple approaches.\n",
    "\n",
    "What we can do, however, is target certain schools. PASSNYC is associated with a lot of organizations that provides services such as:\n",
    "\n",
    "- Test preparation\n",
    "- Afterschool activities\n",
    "- Mentoring\n",
    "- Community and student groups\n",
    "- Etc\n",
    "\n",
    "But, what schools to target? Simple, those with the biggest gap between the actual number of applicants and the expected number of applicants.\n",
    "\n",
    "Okay, not as simple as this, but this can provide a invaluable information in the process of choosing which schools to intervene. This kernel is aimed at providing and simple yet effective model with which this gap can be estimated.\n",
    "\n",
    "[1]: Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Little glimpse at the data\n",
    "\n",
    "The crucial information to what we are doing here is the amount of SHSAT applicants for each school. This information could be retrieved [here][1], but it only includes students from 2017.\n",
    "\n",
    "So, we gathered a fuckton of data about these students and their schools about this period.\n",
    "\n",
    "Information includes:\n",
    "\n",
    "- Percentage of each ethnicity of each school\n",
    "- Percentage of students with disabilities in each school\n",
    "- The Economic Need Index of each school (it indicates the poverty of the students)\n",
    "- Values indicating the distribution of grades for each school at the NYS Common Core tests [2]\n",
    "- Some other things\n",
    "\n",
    "Below you can see what I'm talking about more clearly.\n",
    "\n",
    "[2]: The grades are can be 1, 2, 3 or 4. We have the percentage of each \n",
    "\n",
    "[1]: SHSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "# show head of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "We expect our data to have a lot of collinearity. That is, variables are much related among themselves. This might cause problems when fitting a model.\n",
    "\n",
    "To alleviate this problem, we use a technique called Principal Component Analysis (abbreviated PCA). This technique reduces the amount of features we have to a number we specify. The features generated are those that can best explain the original variables, so, it is a really good approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "\n",
    "To choose the best amount of features, we will use cross-validation. It is a technique for splitting the dataset into training and test sets multiple times, making use of the data in an efficient way.\n",
    "\n",
    "The cross-validation method used here is a repeated k-fold. Explaining it completely is out of the scope of this kernel, but let's just say that it is one of the most recommend methods. Requires a lot of iterations, but, since our dataset is not big (comparing to today's \"big data\"), this is no problem.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html#sklearn.model_selection.RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform cross validation\n",
    "\n",
    "# visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To choose the best number of parameters, we use both the Median Absolute Error and the Mean Squared Error. The first metric indicates how well the model is fitting overall, ignoring the weight of outliers [1]. The second metric, however, is more sensitive to outliers, and gives more importance to big errors.\n",
    "\n",
    "[1]: A better explanation of how we should treat outliers is gonna be made in the modelling section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model\n",
    "\n",
    "Here we will use a very simple model. There are two main gists though:\n",
    "\n",
    "1. We are using it to *measure the application gap* in each school\n",
    "\n",
    "This may seem like not at all relevant to the model choice, but it is, indeed much important.\n",
    "\n",
    "Say, of these two lines, which one do you think works best when predicting the gap between what is expected and what really occured?\n",
    "\n",
    "**show 2 plots of linear regression and an outlier in the y-axis**\n",
    "**two lines, one using usual linear regression and the other using robust regression**\n",
    "\n",
    "I'd say the second one, as it gives a small gap to schools that are \"close to expected\" and predicts a big gap for that school over there that is different from everyone.\n",
    "\n",
    "The first line was generated using an usual regression and the second one used a robust regression (we say robust because it is *robust to outliers*). The model that I'm gonna use is a robust one.\n",
    "\n",
    "2. The outcomes we are trying to predict can be understood as probabilities\n",
    "\n",
    "Okay, they are actually the percentage of applicants of each school. But, can't we assume that this is the probability of each student at a certain school applying for the SHSAT? Although being a simplification, this is what we are gonna work upon. And, being a model to predict probabilities, logistic regression may be a better model than linear regression. [1]\n",
    "\n",
    "---\n",
    "\n",
    "So, we will input the components produces by PCA and expect the model to generate parameters that fit well with our probabilities. The model is gonna be a robust linear regression (with the target variable transformed with the logit function, essentially transforming it into a **robust logistic regression**).\n",
    "\n",
    "**show plot of linear regression and of logistic regression**\n",
    "\n",
    "[1]: Here is a the rationale ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the variables\n",
    "# fit the model\n",
    "# make predictions\n",
    "# get statistic values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summary of the results of our regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low `P>|z|` (which are some sort of a p-value [1]) values indicate that the model is clearly good, the residual plot indicates a healthy fit and the model scores are what we expect, given the previous results from cross-validation.\n",
    "\n",
    "[1]: Is that correct? Some statistician?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a plot for the less statistic-oriented folks. It compare  the percentage of students the were expected to take the test against the percentage that actually happened (schools from 0 to 5 test takers are not included in it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personally, I believe this is a great fit and everyone on Kaggle should use it.... Kidding! I mean, it might probably be of good use to PASSNYC. :)\n",
    "\n",
    "Below I will just calculate the expected probabilities and make them available in an easy format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate probabilities and export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's done...\n",
    "\n",
    "Some more things to do:\n",
    "\n",
    "- Generate a table with predictions and gaps as a prop to PASSNYC\n",
    "- Demonstrate the use of the table alongside an attractiveness score (or link some kernel)\n",
    "- Check to see if the gaps from 2017 are gonna still be relevant for 2018 (central-harlem I call you)\n",
    "- Generate a better model with a little spice that has been in my mind for a long time\n",
    "- Remember the dropped schools! I'm gonna create a model for each category of those bastards.\n",
    "- Anything you post here VVV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
